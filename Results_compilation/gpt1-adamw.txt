============================================================
GPT-1 Training with AdamW Optimizer
============================================================
Model parameters: 6.68M

Optimizer: AdamW
Learning rate: 0.001
Training iterations: 20000
Batch size: 16, Context size: 32

step     0: train loss 10.9975 (ppl=59892.98), val loss 11.0111 (ppl=60499.31)
step   200: train loss 6.0222 (ppl=417.60), val loss 6.1819 (ppl=482.00)
step   400: train loss 5.5045 (ppl=244.45), val loss 5.7195 (ppl=309.27)
step   600: train loss 5.0899 (ppl=163.63), val loss 5.4582 (ppl=231.18)
step   800: train loss 4.8826 (ppl=130.46), val loss 5.2832 (ppl=191.06)
step  1000: train loss 4.7216 (ppl=109.29), val loss 5.1431 (ppl=170.50)
step  1200: train loss 4.6070 (ppl=100.39), val loss 5.0622 (ppl=154.50)
step  1400: train loss 4.5259 (ppl=90.03), val loss 5.0105 (ppl=149.25)
step  1600: train loss 4.4278 (ppl=82.13), val loss 4.9510 (ppl=142.56)
step  1800: train loss 4.3444 (ppl=75.33), val loss 4.9164 (ppl=136.46)
step  2000: train loss 4.2946 (ppl=73.90), val loss 4.9202 (ppl=133.98)
step  2200: train loss 4.2461 (ppl=69.49), val loss 4.8996 (ppl=130.50)
step  2400: train loss 4.1729 (ppl=63.74), val loss 4.8464 (ppl=128.29)
step  2600: train loss 4.1236 (ppl=61.92), val loss 4.8510 (ppl=130.13)
step  2800: train loss 4.0832 (ppl=58.49), val loss 4.8572 (ppl=130.13)
step  3000: train loss 4.0543 (ppl=55.85), val loss 4.8801 (ppl=130.02)
step  3200: train loss 4.0162 (ppl=54.06), val loss 4.8540 (ppl=124.68)
step  3400: train loss 3.9714 (ppl=51.85), val loss 4.8404 (ppl=131.29)
step  3600: train loss 3.9351 (ppl=51.78), val loss 4.8274 (ppl=128.71)
step  3800: train loss 3.9220 (ppl=49.02), val loss 4.8582 (ppl=126.03)
step  4000: train loss 3.8781 (ppl=48.52), val loss 4.9052 (ppl=132.20)
step  4200: train loss 3.8589 (ppl=46.61), val loss 4.8477 (ppl=127.31)
step  4400: train loss 3.8282 (ppl=45.74), val loss 4.8921 (ppl=132.10)
step  4600: train loss 3.8154 (ppl=44.82), val loss 4.8901 (ppl=135.64)
step  4800: train loss 3.7654 (ppl=43.53), val loss 4.9167 (ppl=134.06)
step  5000: train loss 3.7607 (ppl=42.63), val loss 4.9183 (ppl=137.46)
step  5200: train loss 3.7040 (ppl=42.29), val loss 4.8868 (ppl=138.63)
step  5400: train loss 3.6858 (ppl=39.97), val loss 4.9317 (ppl=133.85)
step  5600: train loss 3.6764 (ppl=38.69), val loss 4.9380 (ppl=134.72)
step  5800: train loss 3.6452 (ppl=38.24), val loss 4.9207 (ppl=141.05)
step  6000: train loss 3.6340 (ppl=37.70), val loss 4.9381 (ppl=141.68)
step  6200: train loss 3.6056 (ppl=37.77), val loss 4.9663 (ppl=145.20)
step  6400: train loss 3.5990 (ppl=36.19), val loss 4.9422 (ppl=144.34)
step  6600: train loss 3.5691 (ppl=36.15), val loss 4.9684 (ppl=145.13)
step  6800: train loss 3.5681 (ppl=35.33), val loss 5.0046 (ppl=150.88)
step  7000: train loss 3.5442 (ppl=34.10), val loss 4.9768 (ppl=147.05)
step  7200: train loss 3.5413 (ppl=34.12), val loss 5.0070 (ppl=152.83)
step  7400: train loss 3.5068 (ppl=32.87), val loss 5.0189 (ppl=149.01)
step  7600: train loss 3.4864 (ppl=32.96), val loss 4.9931 (ppl=150.19)
step  7800: train loss 3.4489 (ppl=32.15), val loss 5.0089 (ppl=154.91)
step  8000: train loss 3.4590 (ppl=31.33), val loss 5.0585 (ppl=156.52)
step  8200: train loss 3.4445 (ppl=31.62), val loss 5.0866 (ppl=160.66)
step  8400: train loss 3.4193 (ppl=30.51), val loss 5.0576 (ppl=156.62)
step  8600: train loss 3.4130 (ppl=29.52), val loss 5.0656 (ppl=163.18)
step  8800: train loss 3.3994 (ppl=29.41), val loss 5.0884 (ppl=158.86)
step  9000: train loss 3.3676 (ppl=29.18), val loss 5.1021 (ppl=169.92)
step  9200: train loss 3.3730 (ppl=29.18), val loss 5.1359 (ppl=168.43)
step  9400: train loss 3.3320 (ppl=28.32), val loss 5.1188 (ppl=170.12)
step  9600: train loss 3.3405 (ppl=28.55), val loss 5.1377 (ppl=178.41)
step  9800: train loss 3.3313 (ppl=27.87), val loss 5.1557 (ppl=177.68)
step 10000: train loss 3.3386 (ppl=27.28), val loss 5.1602 (ppl=171.43)
step 10200: train loss 3.3115 (ppl=27.50), val loss 5.1609 (ppl=169.75)
step 10400: train loss 3.2883 (ppl=26.70), val loss 5.1641 (ppl=179.04)
step 10600: train loss 3.2579 (ppl=25.75), val loss 5.1655 (ppl=176.24)
step 10800: train loss 3.2839 (ppl=26.09), val loss 5.1551 (ppl=178.18)
step 11000: train loss 3.2643 (ppl=25.78), val loss 5.2447 (ppl=186.83)
step 11200: train loss 3.2284 (ppl=25.71), val loss 5.2071 (ppl=189.14)
step 11400: train loss 3.2544 (ppl=24.99), val loss 5.2431 (ppl=189.21)
step 11600: train loss 3.2279 (ppl=24.56), val loss 5.2506 (ppl=186.73)
step 11800: train loss 3.1865 (ppl=24.64), val loss 5.2189 (ppl=200.35)
step 12000: train loss 3.1875 (ppl=24.24), val loss 5.2860 (ppl=190.77)
step 12200: train loss 3.1770 (ppl=24.04), val loss 5.2232 (ppl=198.26)
step 12400: train loss 3.1831 (ppl=24.51), val loss 5.2610 (ppl=202.34)
step 12600: train loss 3.1578 (ppl=23.52), val loss 5.2490 (ppl=203.93)
step 12800: train loss 3.1656 (ppl=23.21), val loss 5.2835 (ppl=196.48)
step 13000: train loss 3.1477 (ppl=23.05), val loss 5.3156 (ppl=219.57)
step 13200: train loss 3.1399 (ppl=23.26), val loss 5.3441 (ppl=201.99)
step 13400: train loss 3.1202 (ppl=22.85), val loss 5.3082 (ppl=209.46)
step 13600: train loss 3.1128 (ppl=22.28), val loss 5.3463 (ppl=201.74)
step 13800: train loss 3.1049 (ppl=22.25), val loss 5.3998 (ppl=215.85)
step 14000: train loss 3.1093 (ppl=21.54), val loss 5.3433 (ppl=216.37)
step 14200: train loss 3.0812 (ppl=21.82), val loss 5.3525 (ppl=209.55)
step 14400: train loss 3.0683 (ppl=21.95), val loss 5.3776 (ppl=214.80)
step 14600: train loss 3.0630 (ppl=21.57), val loss 5.4009 (ppl=225.50)
step 14800: train loss 3.0708 (ppl=21.13), val loss 5.3726 (ppl=230.81)
step 15000: train loss 3.0441 (ppl=21.36), val loss 5.4452 (ppl=235.06)
step 15200: train loss 3.0368 (ppl=20.95), val loss 5.4297 (ppl=224.57)
step 15400: train loss 3.0288 (ppl=21.09), val loss 5.4045 (ppl=228.27)
step 15600: train loss 3.0010 (ppl=20.14), val loss 5.4609 (ppl=227.77)
step 15800: train loss 3.0282 (ppl=20.13), val loss 5.4879 (ppl=235.65)
step 16000: train loss 2.9943 (ppl=20.29), val loss 5.4440 (ppl=234.28)
step 16200: train loss 2.9703 (ppl=20.40), val loss 5.5018 (ppl=236.86)
step 16400: train loss 2.9721 (ppl=19.96), val loss 5.4393 (ppl=234.92)
step 16600: train loss 2.9810 (ppl=19.39), val loss 5.4924 (ppl=251.54)
step 16800: train loss 2.9789 (ppl=19.63), val loss 5.5315 (ppl=250.56)
step 17000: train loss 2.9676 (ppl=19.28), val loss 5.5188 (ppl=253.79)
step 17200: train loss 2.9628 (ppl=19.20), val loss 5.5234 (ppl=254.39)
step 17400: train loss 2.9524 (ppl=19.04), val loss 5.5200 (ppl=270.54)
step 17600: train loss 2.9547 (ppl=18.77), val loss 5.5888 (ppl=251.33)
step 17800: train loss 2.9332 (ppl=19.00), val loss 5.5326 (ppl=257.44)
step 18000: train loss 2.9329 (ppl=18.49), val loss 5.5706 (ppl=260.48)
step 18200: train loss 2.9104 (ppl=18.43), val loss 5.6026 (ppl=269.51)
step 18400: train loss 2.9170 (ppl=18.49), val loss 5.6261 (ppl=269.32)
step 18600: train loss 2.8936 (ppl=18.52), val loss 5.6210 (ppl=265.17)
step 18800: train loss 2.9043 (ppl=18.32), val loss 5.6060 (ppl=260.09)
step 19000: train loss 2.8769 (ppl=17.64), val loss 5.6456 (ppl=268.69)
step 19200: train loss 2.8847 (ppl=17.94), val loss 5.7092 (ppl=287.41)
step 19400: train loss 2.8817 (ppl=17.96), val loss 5.6031 (ppl=265.98)
step 19600: train loss 2.8698 (ppl=17.77), val loss 5.6325 (ppl=291.97)
step 19800: train loss 2.8579 (ppl=17.17), val loss 5.6674 (ppl=286.91)
step 19999: train loss 2.8529 (ppl=16.88), val loss 5.6322 (ppl=288.41)

============================================================
Training Complete!
============================================================

============================================================
PERPLEXITY EVALUATION
============================================================
Corpus-Level Perplexity:
  Train: 17.3894
  Val:   290.0470

Per-Sequence Perplexity:
  Train: 21.7167
  Val:   1965.1936
============================================================


Generating sample text...

Generated Text:
------------------------------------------------------------
! what news?

MISTRESS OVERDONE:
What, 'tis no matter, and known, my lord.

DUKE VINCENTIO:
I have done't.

ANGELO:
Nay, she is an irksome weeds,
And strip myself to hear, and our state to death
From kingdom and legs of our noble Gloucester's death.
I am the drudge's eye;
And when I, in the world slip are:
The sessions, whose father, if not at more
Than 'shall have him promise this friend, they
wear for him: but once think there things spoke of him.

Second Citizen:
The time got upon of Angelo; when you do,
Here doth she go mourn with his soul
Of my country, that I do, I should live in,
Which now I think, to sink through the instrument
mourners of rebellion, and let dogs bow his head.
I have been slain within.

Officer:
You are too general: my name, if you say,
You may be told me; and he was a man, and more than
me: the man duke did not be left be much.

Second Servant:
Things will, such a man of that appears he's,
And, though she be a day in him.

Second Murderer:
Ay, by the eyes, base pleasing treaty
With flight and kissing to the fight.

SICINIUS:
The ground, to do so have made thee ill Tybalt's face.

CORIOLANUS:
But, fair lords,
You will not say but this of what I thought.

DORCAS:
Thou hast, no, but thou wilt. Do not Juliet?

BIANCA:
No, my good lord.

Third Servingman:
What, ho! why, man?

Second Murderer:

CLARriven, and he has with a wind;
The generous's, a bawd, a man is trot
one out as in Corioli and bred us
The city ports: if he were a piece of wax.

ANGELO:
This day!

ISABELLA:
O true
------------------------------------------------------------